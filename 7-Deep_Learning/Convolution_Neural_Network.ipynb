{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exterior-girlfriend",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amended-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-parameter",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "african-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-quality",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comic-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator_train = ImageDataGenerator(rotation_range=30,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    rescale=1/255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "image_generator_test = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "personalized-helena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_img_gen = image_generator_train.flow_from_directory('dataset/training_set', \n",
    "                                                    target_size=(64, 64), \n",
    "                                                    batch_size=32, \n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stuffed-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_img_gen = image_generator_test.flow_from_directory('dataset/test_set/',\n",
    "                                                        target_size=(64, 64), \n",
    "                                                        batch_size=32, \n",
    "                                                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ambient-boulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seeing-width",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_gen.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-isaac",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "central-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "general-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "              kernel_size=3,\n",
    "              activation='relu',\n",
    "              input_shape=[64, 64, 3]))\n",
    "\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "              kernel_size=3,\n",
    "              activation='relu'))\n",
    "\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=3))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(3200, activation='relu'))\n",
    "\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "cnn.add(Dense(1600, activation='relu'))\n",
    "\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "cnn.add(Dense(1600, activation='relu'))\n",
    "\n",
    "cnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wireless-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "disabled-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 109s 436ms/step - loss: 1.0561 - accuracy: 0.5576 - val_loss: 0.6775 - val_accuracy: 0.5710\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 109s 435ms/step - loss: 0.6789 - accuracy: 0.6261 - val_loss: 0.6257 - val_accuracy: 0.6250\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.6120 - accuracy: 0.6654 - val_loss: 0.7583 - val_accuracy: 0.6440\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 119s 477ms/step - loss: 0.5920 - accuracy: 0.6915 - val_loss: 0.5648 - val_accuracy: 0.7300\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 118s 471ms/step - loss: 0.5828 - accuracy: 0.6934 - val_loss: 0.5667 - val_accuracy: 0.7090\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 117s 469ms/step - loss: 0.5748 - accuracy: 0.7039 - val_loss: 0.5417 - val_accuracy: 0.7320\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 110s 441ms/step - loss: 0.5648 - accuracy: 0.7139 - val_loss: 0.5655 - val_accuracy: 0.7055\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.5439 - accuracy: 0.7335 - val_loss: 0.5095 - val_accuracy: 0.7400\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 117s 469ms/step - loss: 0.5391 - accuracy: 0.7345 - val_loss: 0.5091 - val_accuracy: 0.7525\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.5307 - accuracy: 0.7374 - val_loss: 0.4878 - val_accuracy: 0.7695\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 117s 469ms/step - loss: 0.5120 - accuracy: 0.7475 - val_loss: 0.4756 - val_accuracy: 0.7735\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 118s 471ms/step - loss: 0.5092 - accuracy: 0.7516 - val_loss: 0.4701 - val_accuracy: 0.7825\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 119s 478ms/step - loss: 0.5009 - accuracy: 0.7551 - val_loss: 0.4964 - val_accuracy: 0.7560\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.5073 - accuracy: 0.7544 - val_loss: 0.4540 - val_accuracy: 0.7855\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 117s 466ms/step - loss: 0.4926 - accuracy: 0.7571 - val_loss: 0.5146 - val_accuracy: 0.7620\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.4919 - accuracy: 0.7606 - val_loss: 0.5503 - val_accuracy: 0.7200\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.4824 - accuracy: 0.7651 - val_loss: 0.7656 - val_accuracy: 0.6505\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 116s 466ms/step - loss: 0.4861 - accuracy: 0.7623 - val_loss: 0.4617 - val_accuracy: 0.7800\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.4675 - accuracy: 0.7810 - val_loss: 0.4637 - val_accuracy: 0.7770\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.4733 - accuracy: 0.7719 - val_loss: 0.5022 - val_accuracy: 0.7715\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 117s 468ms/step - loss: 0.4610 - accuracy: 0.7836 - val_loss: 0.4353 - val_accuracy: 0.8055\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 117s 468ms/step - loss: 0.4634 - accuracy: 0.7772 - val_loss: 0.4263 - val_accuracy: 0.8130\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 117s 466ms/step - loss: 0.4538 - accuracy: 0.7815 - val_loss: 0.4176 - val_accuracy: 0.8180\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 117s 469ms/step - loss: 0.4501 - accuracy: 0.7874 - val_loss: 0.4715 - val_accuracy: 0.7685\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.4447 - accuracy: 0.7956 - val_loss: 0.4134 - val_accuracy: 0.8210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec4680b048>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_img_gen, validation_data=test_img_gen, epochs=25, steps_per_epoch=250, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "every-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('Dog-or-Cat-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-ground",
   "metadata": {},
   "source": [
    "# Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "harmful-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "dog_img = image.img_to_array(image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64)))\n",
    "\n",
    "dog_img = np.expand_dims(dog_img, 0)\n",
    "\n",
    "dog_img = dog_img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "regular-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99923766]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict(dog_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "annual-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img = image.img_to_array(image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size=(64, 64)))\n",
    "\n",
    "cat_img = np.expand_dims(dog_img, 0)\n",
    "\n",
    "cat_img = dog_img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "stuffed-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04867071]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict(cat_img)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
